{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import dm_utils as ut\n",
    "sys.path.append('../')  # the code for fair classification is in this directory\n",
    "\n",
    "import loaders.load_compas_data as compas\n",
    "import loaders.load_bank as bank\n",
    "import loaders.load_adult as adult\n",
    "from dis_meas_class import DispMistreatmentClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dFNR(y_true, y_pred, X, sa_index, sa_label):\n",
    "    sa_pos = (X[:, sa_index] == sa_label) * (y_true == 1)\n",
    "    sa_neg = (X[:, sa_index] == sa_label) * (y_true == -1)\n",
    "    nonsa_pos = (X[:, sa_index] != sa_label) * (y_true == 1)\n",
    "    nonsa_neg = (X[:, sa_index] != sa_label) * (y_true == -1)\n",
    "    return np.sum(y_pred[sa_pos] != y_true[sa_pos]) / np.sum(sa_pos) - np.sum(y_pred[nonsa_pos] != y_true[nonsa_pos]) / np.sum(nonsa_pos)\n",
    "def dFPR(y_true, y_pred, X, sa_index, sa_label):\n",
    "    sa_pos = (X[:, sa_index] == sa_label) * (y_true == 1)\n",
    "    sa_neg = (X[:, sa_index] == sa_label) * (y_true == -1)\n",
    "    nonsa_pos = (X[:, sa_index] != sa_label) * (y_true == 1)\n",
    "    nonsa_neg = (X[:, sa_index] != sa_label) * (y_true == -1)\n",
    "\n",
    "    return np.sum(y_pred[sa_neg] != y_true[sa_neg]) / np.sum(sa_neg) - np.sum(y_pred[nonsa_neg] != y_true[nonsa_neg]) / np.sum(nonsa_neg)\n",
    "def TPR(y_true, y_pred, X, sa_index, sa_label, agg):\n",
    "    sa_pos = (X[:, sa_index] == sa_label) * (y_true == 1)\n",
    "    sa_neg = (X[:, sa_index] == sa_label) * (y_true == -1)\n",
    "    nonsa_pos = (X[:, sa_index] != sa_label) * (y_true == 1)\n",
    "    nonsa_neg = (X[:, sa_index] != sa_label) * (y_true == -1)\n",
    "\n",
    "    if agg == 'prot':\n",
    "        return np.sum(y_pred[nonsa_pos] == y_true[nonsa_pos]) / np.sum(nonsa_pos)\n",
    "    elif agg == 'non-prot':\n",
    "        return np.sum(y_pred[sa_pos] == y_true[sa_pos]) / np.sum(sa_pos)\n",
    "def TNR(y_true, y_pred, X, sa_index, sa_label, agg='diff'):\n",
    "    sa_pos = (X[:, sa_index] == sa_label) * (y_true == 1)\n",
    "    sa_neg = (X[:, sa_index] == sa_label) * (y_true == -1)\n",
    "    nonsa_pos = (X[:, sa_index] != sa_label) * (y_true == 1)\n",
    "    nonsa_neg = (X[:, sa_index] != sa_label) * (y_true == -1)\n",
    "\n",
    "    if agg == 'prot':\n",
    "        return np.sum(y_pred[nonsa_neg] == y_true[nonsa_neg]) / np.sum(nonsa_neg)\n",
    "    elif agg == 'non-prot':\n",
    "        return np.sum(y_pred[sa_neg] == y_true[sa_neg]) / np.sum(sa_neg)\n",
    "def model_evaluate(y_true, y_pred, X, sa_index, sa_label):\n",
    "    metrics = {'Accuracy': accuracy_score(y_true, y_pred), 'Bal. Acc.': balanced_accuracy_score(y_true, y_pred),\n",
    "              'Eq.Odds': abs(dFPR(y_true, y_pred, X, sa_index, sa_label)) + abs(dFNR(y_true, y_pred, X, sa_index, sa_label)),\n",
    "              'TPR Prot': TPR(y_true, y_pred, X, sa_index, sa_label, agg='prot'),\n",
    "              'TPR Non-Prot': TPR(y_true, y_pred, X, sa_index, sa_label, agg='non-prot'),\n",
    "              'TNR Prot': TNR(y_true, y_pred, X, sa_index, sa_label, agg='prot'),\n",
    "              'TNR Non-Prot': TNR(y_true, y_pred, X, sa_index, sa_label, agg='non-prot')}\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n"
     ]
    }
   ],
   "source": [
    "train_fold_size = 0.5\n",
    "constr = 1\n",
    "X, y, sa_index, _, x_control = compas.load_compas()\n",
    "sa_label=1\n",
    "nonprot = X[:, sa_index]==sa_label\n",
    "prot = X[:, sa_index]!=sa_label\n",
    "\n",
    "names = ['Accuracy', 'Bal. Acc.', 'Eq.Odds', 'TPR Prot', 'TPR Non-Prot', 'TNR Prot', 'TNR Non-Prot']\n",
    "metrics = {'name':[], 'value':[], 'model':[]}\n",
    "for i in range(10):\n",
    "    x_train, y_train, \\\n",
    "    x_control_train, \\\n",
    "    x_test, y_test, \\\n",
    "    x_control_test = ut.split_into_train_test(X, y, x_control,train_fold_size)\n",
    "\n",
    "    clf1 = DispMistreatmentClassifier(x_control_train,cons_type=constr)\n",
    "    clf1.fit(x_train, y_train)\n",
    "    y_pred_dm = clf1.predict(x_test)\n",
    "\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_dm))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_dm))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_dm, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_dm, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "\n",
    "    clf2 = RandomForestClassifier(n_estimators=100, random_state=0xB00B1E5)\n",
    "    clf2.fit(x_train, y_train)\n",
    "    y_pred_rf = clf2.predict(x_test)\n",
    "\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_rf))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_rf))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_rf, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_rf, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "\n",
    "    clf3 = LogisticRegression(random_state=0xB00B1E5)\n",
    "    clf3.fit(x_train, y_train)\n",
    "\n",
    "    y_pred_lr = clf3.predict(x_test)\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_lr))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_lr))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_lr, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_lr, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('LogisticRegression')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% compass\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = open('metrics_compass.txt','w')\n",
    "f.write(str(metrics))\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n"
     ]
    }
   ],
   "source": [
    "train_fold_size = 0.4\n",
    "X, y, sa_index, _, x_control = adult.load_adult(\"sex\")\n",
    "sa_label = 0\n",
    "nonprot = X[:, sa_index]==sa_label\n",
    "prot = X[:, sa_index]!=sa_label\n",
    "\n",
    "names = ['Accuracy', 'Bal. Acc.', 'Eq.Odds', 'TPR Prot', 'TPR Non-Prot', 'TNR Prot', 'TNR Non-Prot']\n",
    "metrics = {'name':[], 'value':[], 'model':[]}\n",
    "for i in range(10):\n",
    "    x_train, y_train, \\\n",
    "    x_control_train, \\\n",
    "    x_test, y_test, \\\n",
    "    x_control_test = ut.split_into_train_test(X, y, x_control,train_fold_size)\n",
    "\n",
    "    clf1 = DispMistreatmentClassifier(x_control_train,cons_type=constr)\n",
    "    clf1.fit(x_train, y_train)\n",
    "    y_pred_dm = clf1.predict(x_test)\n",
    "\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_dm))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_dm))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_dm, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_dm, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "\n",
    "    clf2 = RandomForestClassifier(n_estimators=100, random_state=0xB00B1E5)\n",
    "    clf2.fit(x_train, y_train)\n",
    "    y_pred_rf = clf2.predict(x_test)\n",
    "\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_rf))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_rf))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_rf, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_rf, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "\n",
    "    clf3 = LogisticRegression(random_state=0xB00B1E5,max_iter=1000)\n",
    "    clf3.fit(x_train, y_train)\n",
    "\n",
    "    y_pred_lr = clf3.predict(x_test)\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_lr))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_lr))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_lr, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_lr, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('LogisticRegression')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Adult\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "f = open('metrics_adult.txt','w')\n",
    "f.write(str(metrics))\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n",
      "Optimization done, problem status: Converged\n"
     ]
    }
   ],
   "source": [
    "X, y, sa_index, p_Group, x_control = bank.load_bank()\n",
    "sa_label=1\n",
    "nonprot = X[:, sa_index]==sa_label\n",
    "prot = X[:, sa_index]!=sa_label\n",
    "train_fold_size = 0.5\n",
    "\n",
    "names = ['Accuracy', 'Bal. Acc.', 'Eq.Odds', 'TPR Prot', 'TPR Non-Prot', 'TNR Prot', 'TNR Non-Prot']\n",
    "metrics = {'name':[], 'value':[], 'model':[]}\n",
    "for i in range(10):\n",
    "    x_train, y_train, \\\n",
    "    x_control_train, \\\n",
    "    x_test, y_test, \\\n",
    "    x_control_test = ut.split_into_train_test(X, y, x_control,train_fold_size)\n",
    "\n",
    "    clf1 = DispMistreatmentClassifier(x_control_train,cons_type=constr)\n",
    "    clf1.fit(x_train, y_train)\n",
    "    y_pred_dm = clf1.predict(x_test)\n",
    "\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_dm))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_dm))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_dm, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_dm, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_dm, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('DispMist_FPR')\n",
    "\n",
    "    clf2 = RandomForestClassifier(n_estimators=100, random_state=0xB00B1E5)\n",
    "    clf2.fit(x_train, y_train)\n",
    "    y_pred_rf = clf2.predict(x_test)\n",
    "\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_rf))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_rf))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_rf, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_rf, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_rf, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('RandomForest')\n",
    "\n",
    "    clf3 = LogisticRegression(random_state=0xB00B1E5)\n",
    "    clf3.fit(x_train, y_train)\n",
    "\n",
    "    y_pred_lr = clf3.predict(x_test)\n",
    "    metrics['name'].append(names[0])\n",
    "    metrics['value'].append(accuracy_score(y_test, y_pred_lr))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[1])\n",
    "    metrics['value'].append(balanced_accuracy_score(y_test, y_pred_lr))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[2])\n",
    "    metrics['value'].append(abs(dFPR(y_test, y_pred_lr, x_test, sa_index, sa_label)) + abs(dFNR(y_test, y_pred_lr, x_test, sa_index, sa_label)))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[3])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[4])\n",
    "    metrics['value'].append(TPR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[5])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='prot'))\n",
    "    metrics['model'].append('LogisticRegression')\n",
    "    metrics['name'].append(names[6])\n",
    "    metrics['value'].append(TNR(y_test, y_pred_lr, x_test, sa_index, sa_label, agg='non-prot'))\n",
    "    metrics['model'].append('LogisticRegression')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Bank\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "f = open('metrics_bank.txt','w')\n",
    "f.write(str(metrics))\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_cm = confusion_matrix(y_test, y_pred_rf)\n",
    "rf_disp = ConfusionMatrixDisplay(confusion_matrix=rf_cm)\n",
    "rf_disp.plot()\n",
    "plt.show()\n",
    "\n",
    "dm_cm = confusion_matrix(y_test, y_pred_dm)\n",
    "dm_disp = ConfusionMatrixDisplay(confusion_matrix=dm_cm)\n",
    "dm_disp.plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ae3c6d58",
   "language": "python",
   "display_name": "PyCharm (fair_classification)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}